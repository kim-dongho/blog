---
title: Percolator - Understanding and Analyzing Distributed Transactions
date: "2020-08-12"
tags:
  [
    "database",
    "paper reading",
    "transaction",
    "distributed system",
    "distributed system44",
  ]
draft: false
summary: This document analyzes the distributed transaction protocol used in Percolator, a system for large-scale incremental processing deployed by Google. Percolator extends Bigtable to provide multi-row ACID transactions through a standard two-phase commit protocol.
---

<a name="Gln9B"></a>

### 概述

一个 web 页面能不能被 Google 搜索到，取决于它是否被 Google 抓取并存入了它的倒排索引。Google 管理着万亿级别的倒排索引，并且每天都有着几十亿级别的数据更新。通过 MapReduce 批处理，可以高效地将整个数据库全量构建成倒排索引。但是对于增量数据来说，全量构建是不经济且不及时的。所以 Google 研发了 Percolator，用于处理这些大规模的增量数据。
为了确保系统故障时的数据一致性，Percolator 在 BigTable 的基础之上，提供了 Snapshot Isolation 语义的分布式事务能力。本文讨论的重点就在于：Percolator 如何基于 BigTable 的单行事务和一个 Timestamp Oracle（全局授时服务器，后称 TSO）实现上述的分布式事务。

> 2022/9/13 补充
> 相关背景：
> 使用 MapReduce 的话，需要为了一小部分新数据全量计算一遍，很慢且不经济
> 现存的 DBMS 无法存储 Google 的巨量数据
>
> BigTable 之类的 NewSQL 可以存储巨量数据，但是缺少跨行事务机制
> 综上，最简单的方案就是为 BigTable 增加分布式事务机制。
>
> Observer 机制：
> 开发者可以创建 observer（就是一段代码），当数据发生变化时会触发 observer 运行。observer 可以读写数据，然后触发后续 observer。一个典型的 Percolator 程序就是一系列的 observer。

<a name="O9810"></a>

### 模型

Percolator 实现分布式事务主要基于 3 个实体：Client、TSO、BigTable。

- Client 是事务的发起者和协调者
- TSO 为分布式服务器提供一个精确的，严格单调递增的时间戳服务。
- [BigTable](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf)是 Google 实现的一个多维持久化 Map，格式如下：<br />
  `(row:string, column:string, timestamp:int64)-> value:string`

实际上 Percolator 存储一列数据的时候，会在 BigTable 中存储多列数据：

- data 列： 存储 value
- lock 列： 存储用于分布式事务的锁信息
- write 列：存储用于分布式事务的提交时间（commit_timestamp）

![image.png](/assets/hh8fv9/table1.png)

对于上表来说，它表示的是 k1 这一行，c1 这一列数据的 3 个版本（timestamp 分别为 5、6、7）。
因为 write 列中，最新的数据是 data@5，表明最新的一次 commit_timestamp 为 5。查询 data 列可知，timestamp=5 时，数据为 foobar。所以 k1 行 c1 列最新数据为：foobar。
在 timestamp=7 的时候，data 列的数据被改为了 barbaz，lock 列也被标记为锁定，意味着此时有一个事务正在修改数据。一旦这个数据成功提交，在未来个某个 timestamp 上，会将它的 write 列写为 7，并把 7 中的 lock 列清理掉。

可以看出，Percolator 使用了 lock 和 write 这额外的 2 列来表达事务的状态。下面会具体说明，如何使用这 2 列作为元数据来实现分布式事务。

> 2022/9/14 补充：
> Percolator 的事务模型是一个将 coodinator 角色 offload 到 client 端的 2PC 算法。由于 client 不具备高可用性，它就只能将一部分元数据存储到存储引擎端。
> 要理解这个算法，主要是要理解它的元数据结构和作用、以及如何实现 Snapshot Isolation。

<a name="WPOEv"></a>

### “写”事务

Percolator 的分布式写事务是由 2 阶段提交（后称 2PC）实现的。不过它对传统 2PC 做了一些修改，可见后文的“Percolator 特色的 2PC”。
一个写事务可以包含多个写操作，事务开启时，Client 会从 TSO 处获取一个 timestamp 作为事务的开始时间（后称为 start_ts）。在提交之前，所有的写操作都只是缓存在内存里。提交时会经过 prewrite 阶段和 commit 阶段。 <a name="uGP2U"></a>

#### Prewrite 阶段

1. 随机取一个写操作作为 primary，其他的写操作则自动成为 secondary。Percolator 总是先操作 primary，“Percolator 特色的 2PC”会讨论到为什么总是先操作 primary。
2. 冲突检测：
   1. 如果在 start_ts 之后，发现 write 列有数据，则说明有其他事务在当前事务开始之后提交了。这相当于 2 个事务的并发写冲突，所以需要将当前事务 abort。
   2. 如果在任何 timestamp 上发现 lock 列有数据，则有可能有其他事务正在修改数据，也将当前事务 abort。当然也有可能另一个事务已经因为崩溃而失败了，但 lock 列还在，对于这一问题后面的故障恢复会讨论到。
3. 锁定和写入：对于每一行每一列要写入的数据，先将它锁定（通过标记 lock 列，secondary 的 lock 列会指向 primary），然后将数据写入它的 data 列（timestamp 就是 start_ts）。此时，因为该数据的 write 列还没有被写入，所以其他事务看不到这次修改。

对于同一个写操作来说，data、lock、write 列的修改由 BigTable 单行事务保证事务性。

> 由冲突检测的 b 可以推测：如果有多个并发的大事务，并且操作的数据有重合，则可能会频繁 abort 事务，这会是一个问题。在 TiDB 的改进中会谈到它们怎么解决这一问题。

<a name="DlflC"></a>

#### Commit 阶段

1. 从 TSO 处获取一个 timestamp 作为事务的提交时间（后称为 commit_ts）。
2. 提交 primary, 如果失败，则 abort 事务：
   1. 检查 primary 上的 lock 是否还存在，如果不存在，则 abort。（其他事务有可能会认为当前事务已经失败，从而清理掉当前事务的 lock）
   2. 以 commit_ts 为 timestamp, 写入 write 列，value 为 start_ts。清理 lock 列的数据。注意，此时为 Commit Point。“写 write 列”和“清理 lock 列”由 BigTable 的单行事务保证 ACID。
3. 一旦 primary 提交成功，则整个事务成功。此时已经可以给客户端返回成功了。secondary 的数据可以异步的写入，即便发生了故障，此时也可以通过 primary 中数据的状态来判断出 secondary 的结果。具体分析可见故障恢复。

<a name="p0LLF"></a>

### “读”事务

> 在早期的 SQL 标准中定义的四个事务隔离级别，只适用于基于锁的事务并发控制。后来论文 A Critique of ANSI SQL Isolation Levels 批判 SQL 标准对隔离级别的定义，并在论文里提到了一种新的隔离级别 —— Snapshot Isolation。在 Snapshot Isolation 下，不会出现脏读、不可重复度和幻读的问题。并且读操作不会被阻塞，对于读多写少的应用，Snapshot Isolation 是非常好的选择。所以，主流数据库都实现了 Snapshot Isolation。更多关于 Snapshot Isolation 的内容和历史，可以查阅《Design Data-Intensive Applications》第 7 章第 2 节。

事务隔离级别，实际上规定的就是事务之间的可见性。对于 Snapshot Isolation 来说：事务只能看到早于它开始时刻之前提交的其他事务。
以下图为例：事务 2 看不到事务 1 的修改，因为事务 1 在事务 2 开始之后才提交；事务 3 可以看到事务 1 和事务 2 的修改，因为它们在事务 3 开始之前提交。PS：因为事务 1 和事务 2 是并发的，如果它们操作的数据有重合，则至少有一个事务会回滚。
![image.png](/assets/hh8fv9/1594914069442-b8c398e0-58a4-4f44-96de-1cb59c54ea34.png)
精确地获取事务的 start_ts 和 commit_ts 是很重要的。因为它关系着事务之间的可见行，以及决定了多个事务是否并行。Percolator 使用单点的 Timestamp Oracle 模块来提供 start_ts 和 commit_ts。

<a name="XhusC"></a>

#### 分布式下 snapshot-read 存在的问题

尽管我们可以通过 TSO 获取到精确的 start_ts 和 commit_ts，但还是可能出现一个小问题：事务实际执行的顺序可能和时间戳的顺序不一致！
举个例子：T1 的 commit_ts < T2 的 start_ts， T2 执行 Read 的时候，T1 还没 commit 完成。按照标准，T2 应该需要可以读取到 T1 的更改，但实际上因为 T1 还没有 commit。

| T1                | T2           |
| ----------------- | ------------ |
| Prewrite （row1） |              |
| Get commit_ts     |              |
|                   | Get start_ts |
|                   | Read (row1)  |
| Commit            |              |

Percolator 的解法是：

1. 让“prewrite 阶段”先于（happens-before）“获取 commit_ts”。所以在 commit_ts 之后，prewrite 的数据必然被锁定了。
2. 如果读取时，发现当前数据已被锁定（锁定意味着其他写事务正在执行），则等待并重试。当然也有可能另一个事务已经崩溃了，后面会谈到。

对于上面的例子来说，T2 执行到 Read 时，发现 row1 被 T1 锁住了，就会等待直到 T1 commit 完成。这样 T2 就能读取到 T1 commit 的结果了，符合了 snapshot-read 的标准。

<a name="9j35d"></a>

### Percolator 特色的 2PC

不同于传统意义上的 2PC，在 Percolator 中貌似看不到 Transactional Coordinator 的角色。其实只要是 2PC 都需要 Coordinator，只是 Percolator 把 Coordinator 的职责作了更进一步的细分，从而不再需要一个中心化的节点。
在 2PC 中，最关键的莫过于 Commit Point（提交点）。因为在 Commit Point 之前，事务都不算生效，并且随时可以回滚。而一旦过了 Commit Point，事务必须生效，哪怕是发生了网络分区、机器故障，一旦恢复都必须继续下去。
传统的 2PC 的 Commit Point 在写本地磁盘的那一刻；Percolator 2PC 的 Commit Point 在完成 BigTable 事务的那一刻。为什么会有这样的区别？因为传统的 Coordinator 是可以做高可用的，参与者可以在 Coordinator 恢复后去询问事务的结果，而 Percolator Client 无法做到高可用，所以将事物状态记录在 client 端是没有意义的。

![image.png](/assets/hh8fv9/table2.png)

PS：至于上文提到的，Percolator 总是先操作 primary，最重要的原因在于 Percolator 把“写 primary 的 write 列、清理 lock 列”作为了 commit point。

<a name="F0EOs"></a>

### 故障恢复

首先明确一点：一个事务是否执行成功，只取决于 Commit Point。一旦一个事务的 Commit Point 确定，所有写操作都最终**必须确定**且**必须一致**地接受。传统 2PC 从 Coordinator 处获取全局最终一致性，而 Percolator 的 2PC 就只能从 data、lock、write 这 3 列的状态来判断全局最终一致性了。**如何从这几列数据判断出事务的提交状态，就是问题的关键所在。**

<a name="SN9u1"></a>

#### 事务崩溃的边界如何划分？

判断出事务的提交状态后，接着就是故障恢复。Percolator 使用了懒处理的方式。一个事务执行时，会判断先前事务的状态，如果发现先前的事务故障了，则帮助它进行相应的故障恢复。这边是没有办法 100%确定某个事务崩溃的，比如事务 A 因为网络分区而阻塞了，那阻塞多久算事务 A 失败呢？**量变和质变的边界不是 100%清晰。或者说，量变到质变的转化不是客观存在的，而是由第三方事务来决定的！**Percolator 只能猜测（原文 suspect）一个事务失败，从而对它进行 abort 或者 rollback/roll-forward。同时 Percolator 也采用了一个 lightweight lock service 来使这个猜测过程更迅速。
PS：回头可以发现，Percolator 2PC 的 commit 阶段，会先判断 primary 上的锁是否还在。就是因为任意事务可能会被其他事务认为已经崩溃了，从而被 abort。 <a name="BjM8w"></a>

#### 如何判断事务的最终状态

想象一下事务 B 正在执行，它要操作的一行数据只有可能是 2 种状态：有锁和无锁。有锁状态下，一定是有其他事务 A 并行，事务 A 可能正在运行，也有可能崩溃了。无锁状态下，可能是无其他事务并行，也有可能是事务崩溃并且锁已经被其他事务清理掉了。
PS：这里的有锁和无锁，全部指代的是 primary 的 lock 列是否有数据。如果事务 B 操作的是 secondary，需要根据 secondary 的 lock 列寻址到相应的 primary 的 lock 列。
如下图：
![image.png](/assets/hh8fv9/1594968348275-7455ae99-356a-4302-8c84-a33af2d97323.png)

1. 事务 B 发现有锁，且该锁未超时，则**认为**事务 A 正在执行。
2. 事务 B 发现有锁，且该锁超时，则**认为**事务 A 已 crash。此时事务 B 可以将事务 A rollback，并继续自己的事务。如果后续事务 A 又恢复了，它在提交前一定会检查自己的 primary lock 是否在存在。所以事务 A 和 B 永远只有一个能够提交成功。
3. 事务 B 发现无锁，则可能无并发事务、或者事务 A 已 crash 但已被事务 C 清理。

<a name="albvq"></a>

### TiDB 的对“写-写冲突”的改进

首先 TiDB 提供了良好的控制台工具，可以查看数据库的事务冲突数量、事务重试数量以及事务延迟。根据告警可以定位到数据，从而找到对应的代码，最终可以尝试通过修改代码解决问题。TiDB 在 v3.0.8 之后也提供了悲观事务模型：会在每个 DML 语句执行的时候，加上悲观锁，用于防止其他事务修改相同 Key，从而保证在最后提交的 prewrite 阶段不会出现写写冲突的情况。

原先 Percolator 的 DML 语句只是缓存在内存里，直到 Prewrite 阶段才会进行冲突检测和加锁：
![image.png](/assets/hh8fv9/1594974054312-4c2b82a3-0133-435f-a6bc-3bf8cbd153e5.png)

> 对于乐观锁的实现，TiDB 官方描述如下：在悲观事务模型下，DML 语句在最开始就必须进行冲突检测和加锁，这里的悲观锁的格式和乐观事务中的锁几乎一致，但是锁的内容是空的，只是一个占位符，待到 Commit 的时候，直接将这些悲观锁改写成标准的 Percolator 模型的锁，后续流程和原来保持一致即可。对于读请求，遇到这类悲观锁的时候，不用像乐观事务那样等待解锁，可以直接返回最新的数据即可（至于为什么，读者可以仔细想想）。

我的想法是：
在乐观模型下，Read 看到锁必须等待是因为，Read 不能确定是否会有一个 commit_ts 小于它 start_ts 的事务，会在它读取之后提交。
但在悲观模型下，Read 看到的 lock 列可能只是一个占位符，也可能是 Percolator 模型的锁。如果是一个占位符的话，那表示此时该事务的 commit_ts 必定还没有获取，所以无需等待该事务。如果是 Percolator 模型的锁的话，则还需要像原本一样处理。
![image.png](/assets/hh8fv9/1594974088233-e6ae7843-f88b-4f10-9d01-79397514b47a.png)

为了避免悲观事务模型引入的死锁，TiDB 实现了一个死锁检测机制。事务 1 对 A 上了锁后，如果另外一个事务 2 对 A 进行等待，那么就会产生一个依赖关系：事务 2 依赖事务 1，如果此时事务 1 打算去等待 B（假设此时事务 2 已经持有了 B 的锁），那么死锁检测模块就会发现一个循环依赖，然后中止（或者重试）这个事务就好了，因为这个事务并没有实际的 prewrite 和 commit，所以这个代价是比较小的。

<a name="yscyN"></a>

### 总结

Percolator 基于 BigTable 单行事务实现的分布式事务，其实是一个乐观事务模型。只有在事务提交时，才会检测写-写冲突。Percolator 事务模型的优点在于原理简单方便理解，不再需要一个中心化的单独 Coordinator，而是把 Coordinator 角色的职责进行细分，把能持久化的部分交给 BigTable 处理，后续也不再依赖 Client 的恢复。但它的缺点也是显而易见的：Client 和 BigTable 之间的 RPC、BigTable 和 ChunkServer 之间的 RPC 都会比较耗费网络资源。TSO 是一个中心化的点。并发事务很多的时候，会占用很多内存。并发大事务可能会频繁冲突，而重试有可能会导致雪崩效应（这时候就用悲观事务模型会更好）。懒处理事务 crash 导致一个事务的延迟可能会比较高。
不过 Percolator 论文只是指明了大的方向，很多细节应该也还是存在优化空间的。

<a name="U8x9W"></a>

### 彩蛋

在 Percolator 中，start_ts 在**事务最开始**的时候获取，而 Snapshot Isolation 隔离级别决定了该事务只能看到 commit_ts 先于 start_ts 的事务的修改。

于是对于 TiDB 来说，下面 2 个并行的事务，T2 能看到的记录只有 5 行，因为它的 start_ts 早与 T1 的 commit_ts。
但是对于 MySQL 来说，它能看到 6 行。因为它的 start_ts 开始于第一条 SQL 语句执行的那一刻。

![image.png](/assets/hh8fv9/table3.png)

### 参考资料

1. [Large-scale Incremental Processing Using Distributed Transactions and Notifications](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36726.pdf)
2. [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/zh-CN//archive/bigtable-osdi06.pdf)
3. [Designing Data-Intensive Applications](https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321)
4. [Google Percolator 分布式事务实现原理解读](http://mysql.taobao.org/monthly/2018/11/02/)
5. [TiDB 新特性漫谈：悲观事务](https://pingcap.com/blog-cn/pessimistic-transaction-the-new-features-of-tidb/)
6. [Percolator 和 TiDB 事务算法](https://pingcap.com/blog-cn/percolator-and-txn/)
7. [Percolator.pdf](https://www.yuque.com/attachments/yuque/0/2022/pdf/577720/1663212034815-0dd603a2-1f08-46ac-9927-fd3dc79aa06a.pdf?_lake_card=%7B%22src%22%3A%22https%3A%2F%2Fwww.yuque.com%2Fattachments%2Fyuque%2F0%2F2022%2Fpdf%2F577720%2F1663212034815-0dd603a2-1f08-46ac-9927-fd3dc79aa06a.pdf%22%2C%22name%22%3A%22Percolator.pdf%22%2C%22size%22%3A298129%2C%22type%22%3A%22application%2Fpdf%22%2C%22ext%22%3A%22pdf%22%2C%22source%22%3A%22%22%2C%22status%22%3A%22done%22%2C%22download%22%3Atrue%2C%22taskId%22%3A%22ud1e74c29-a692-4093-92b1-13cfeb94c2f%22%2C%22taskType%22%3A%22upload%22%2C%22__spacing%22%3A%22both%22%2C%22id%22%3A%22uf7eea244%22%2C%22margin%22%3A%7B%22top%22%3Atrue%2C%22bottom%22%3Atrue%7D%2C%22card%22%3A%22file%22%7D)
